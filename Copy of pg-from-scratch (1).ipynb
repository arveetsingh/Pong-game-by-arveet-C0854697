{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of pg-from-scratch.ipynb","provenance":[{"file_id":"https://github.com/DJCordhose/ai/blob/master/notebooks/rl/pg-from-scratch.ipynb","timestamp":1660933526114}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Pong game with varying neurons and learning rate**"],"metadata":{"id":"UNX6wwZrGngw"}},{"metadata":{"id":"uF9MAVI16huj"},"cell_type":"code","source":["# !apt-get install python-opengl -y  >/dev/null\n","# !apt install xvfb -y >/dev/null"],"execution_count":null,"outputs":[]},{"metadata":{"id":"fSC11TfN6p69"},"cell_type":"code","source":["# !pip install pyvirtualdisplay >/dev/null\n","# !pip install piglet >/dev/null"],"execution_count":null,"outputs":[]},{"metadata":{"id":"caiHE2hy6xrf"},"cell_type":"code","source":["# from pyvirtualdisplay import Display\n","# display = Display(visible=0, size=(1400, 900))\n","# display.start()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"cWACPRL869I4"},"cell_type":"code","source":["!pip install gym >/dev/null"],"execution_count":null,"outputs":[]},{"metadata":{"id":"2Os6feRY6ec_"},"cell_type":"code","source":["!pip install JSAnimation >/dev/null"],"execution_count":null,"outputs":[]},{"metadata":{"id":"wotUOa_e6edP"},"cell_type":"code","source":["%matplotlib inline\n","from JSAnimation.IPython_display import display_animation\n","from matplotlib import animation\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML\n","\n","def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n","    display(display_animation(anim, default_mode='once'))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"R66_INeZ9nYX"},"cell_type":"markdown","source":["## Step 2: Playing Pong"]},{"metadata":{"id":"MtT2GyK_6edc","outputId":"5d6284d2-c12e-4162-f021-1fe1c6cc8a60","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660933306600,"user_tz":240,"elapsed":28547,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["%pip install -U gym>=0.21.0\n","%pip install -U gym[atari,accept-rom-license]\n","import gym\n","env = gym.make('Pong-v0')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.1)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n","Collecting ale-py~=0.7.5\n","  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 5.1 MB/s \n","\u001b[?25hCollecting autorom[accept-rom-license]~=0.4.2\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n","Collecting AutoROM.accept-rom-license\n","  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=487a64f0eea4e5aeb2252be6d13df2b4db70a9b1c799e2dc9ed3fe3240281666\n","  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n","Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  f\"The environment {id} is out of date. You should consider \"\n","/usr/local/lib/python3.7/dist-packages/gym/core.py:330: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"]}]},{"metadata":{"id":"oRE6WmXQJ1Z0","outputId":"1f0c4101-314c-446b-bb4c-08fa21d965fd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660933306609,"user_tz":240,"elapsed":105,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["env.action_space"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discrete(6)"]},"metadata":{},"execution_count":11}]},{"metadata":{"id":"yl_9d4HFJ31W","outputId":"85f57ba4-5f2f-426a-efd0-02eaff76e451","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660933306611,"user_tz":240,"elapsed":92,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["env.observation_space"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(0, 255, (210, 160, 3), uint8)"]},"metadata":{},"execution_count":12}]},{"metadata":{"id":"trwRXI-h6eeI","outputId":"52da0a04-076c-4061-adb4-06a861178ad6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660933307353,"user_tz":240,"elapsed":811,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["# Run a demo of the environment\n","observation = env.reset()\n","cumulated_reward = 0\n","\n","frames = []\n","for t in range(1000):\n","#     print(observation)\n","    frames.append(env.render(mode = 'rgb_array'))\n","    # very stupid agent, just makes a random action within the allowd action space\n","    action = env.action_space.sample()\n","#     print(\"Action: {}\".format(t+1))    \n","    observation, reward, done, info = env.step(action)\n","#     print(reward)\n","    cumulated_reward += reward\n","    if done:\n","        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n","        break\n","print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n","\n","env.close()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/core.py:52: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  \"The argument mode in render method is deprecated; \"\n","/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n","  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n","/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  \"Core environment is written in old step API which returns one bool instead of two. \"\n"]},{"output_type":"stream","name":"stdout","text":["Episode finished without success, accumulated reward = -15.0\n"]}]},{"metadata":{"id":"qUNldrqa6eeM"},"cell_type":"code","source":[""],"execution_count":null,"outputs":[]},{"metadata":{"id":"3zZTecVWLLes"},"cell_type":"code","source":["def sigmoid(x): \n","  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n","\n","def prepro(I):\n","  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n","  I = I[35:195] # crop\n","  I = I[::2,::2,0] # downsample by factor of 2\n","  I[I == 144] = 0 # erase background (background type 1)\n","  I[I == 109] = 0 # erase background (background type 2)\n","  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n","  return I.astype(np.float).ravel()\n","\n","def policy_forward(x):\n","  h = np.dot(model['W1'], x)\n","  h[h<0] = 0 # ReLU nonlinearity\n","  logp = np.dot(model['W2'], h)\n","  p = sigmoid(logp)\n","  return p, h # return probability of taking action 2, and hidden state\n","\n","def model_step(model, observation, prev_x):\n","  # preprocess the observation, set input to network to be difference image\n","  cur_x = prepro(observation)\n","  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","  prev_x = cur_x\n","  \n","  # forward the policy network and sample an action from the returned probability\n","  aprob, _ = policy_forward(x)\n","  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n","  \n","  return action, prev_x\n","\n","def play_game(env, model):\n","  observation = env.reset()\n","\n","  frames = []\n","  cumulated_reward = 0\n","\n","  prev_x = None # used in computing the difference frame\n","\n","  for t in range(1000):\n","      frames.append(env.render(mode = 'rgb_array'))\n","      action, prev_x = model_step(model, observation, prev_x)\n","      observation, reward, done, info = env.step(action)\n","      cumulated_reward += reward\n","      if done:\n","          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n","          break\n","  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n","  env.close()\n","  display_frames_as_gif(frames)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"6gWvZQ7AQLQt"},"cell_type":"markdown","source":["## Step 3: Policy Gradient from Scratch"]},{"metadata":{"id":"eqFm7hqcItWl"},"cell_type":"code","source":["import numpy as np\n","\n","# model initialization\n","H = 400 # number of hidden layer neurons\n","D = 80 * 80 # input dimensionality: 80x80 grid\n","model = {}\n","model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n","model['W2'] = np.random.randn(H) / np.sqrt(H)\n","\n","# import pickle\n","# model = pickle.load(open('model.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"IJ4SeY8tPxvn"},"cell_type":"code","source":[""],"execution_count":null,"outputs":[]},{"metadata":{"id":"TwjiwKisQM19"},"cell_type":"code","source":["# hyperparameters\n","batch_size = 10 # every how many episodes to do a param update?\n","# learning_rate = 1e-4\n","learning_rate = 1e-3\n"," \n","gamma = 0.99 # discount factor for reward\n","decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n","  \n","grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n","rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n","\n","def discount_rewards(r):\n","  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n","  discounted_r = np.zeros_like(r, dtype=np.float32)\n","  running_add = 0\n","  for t in reversed(range(0, r.size)):\n","    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n","    running_add = running_add * gamma + r[t]\n","    discounted_r[t] = running_add\n","  return discounted_r\n","\n","def policy_backward(epx, eph, epdlogp):\n","  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n","  dW2 = np.dot(eph.T, epdlogp).ravel()\n","  dh = np.outer(epdlogp, model['W2'])\n","  dh[eph <= 0] = 0 # backpro prelu\n","  dW1 = np.dot(dh.T, epx)\n","  return {'W1':dW1, 'W2':dW2}\n","\n","def train_model(env, model, total_episodes = 100):\n","  hist = []\n","  observation = env.reset()\n","\n","  prev_x = None # used in computing the difference frame\n","  xs,hs,dlogps,drs = [],[],[],[]\n","  running_reward = None\n","  reward_sum = 0\n","  episode_number = 0\n","\n","  while True:\n","    # preprocess the observation, set input to network to be difference image\n","    cur_x = prepro(observation)\n","    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","    prev_x = cur_x\n","\n","    # forward the policy network and sample an action from the returned probability\n","    aprob, h = policy_forward(x)\n","    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n","\n","    # record various intermediates (needed later for backprop)\n","    xs.append(x) # observation\n","    hs.append(h) # hidden state\n","    y = 1 if action == 2 else 0 # a \"fake label\"\n","    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n","\n","    # step the environment and get new measurements\n","    observation, reward, done, info = env.step(action)\n","    reward_sum += reward\n","\n","    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n","\n","    if done: # an episode finished\n","      episode_number += 1\n","\n","      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n","      epx = np.vstack(xs)\n","      eph = np.vstack(hs)\n","      epdlogp = np.vstack(dlogps)\n","      epr = np.vstack(drs)\n","      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n","\n","      # compute the discounted reward backwards through time\n","      discounted_epr = discount_rewards(epr)\n","      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n","      discounted_epr -= np.mean(discounted_epr)\n","      discounted_epr /= np.std(discounted_epr)\n","\n","      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n","      grad = policy_backward(epx, eph, epdlogp)\n","      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n","\n","      # perform rmsprop parameter update every batch_size episodes\n","      if episode_number % batch_size == 0:\n","        for k,v in model.items():\n","          g = grad_buffer[k] # gradient\n","          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n","          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n","          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n","\n","      # boring book-keeping\n","      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n","      hist.append((episode_number, reward_sum, running_reward))\n","      plt.scatter(episode_number, running_reward, marker=\".\")\n","      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n","      \n","      reward_sum = 0\n","      observation = env.reset() # reset env\n","      prev_x = None\n","      if episode_number == total_episodes: \n","        return hist\n","        return plt.show()\n","\n","  #   if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n","  #     print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"G6Ka_5Vl9Orm","outputId":"dbb40a4c-10f2-433a-8f27-d836c84721ad","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1660933520892,"user_tz":240,"elapsed":213374,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["%time hist1 = train_model(env, model, total_episodes=500)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["resetting env. episode 1.000000, reward total was -20.000000. running mean: -20.000000\n","resetting env. episode 2.000000, reward total was -21.000000. running mean: -20.010000\n","resetting env. episode 3.000000, reward total was -21.000000. running mean: -20.019900\n","resetting env. episode 4.000000, reward total was -20.000000. running mean: -20.019701\n","resetting env. episode 5.000000, reward total was -20.000000. running mean: -20.019504\n","resetting env. episode 6.000000, reward total was -21.000000. running mean: -20.029309\n","resetting env. episode 7.000000, reward total was -21.000000. running mean: -20.039016\n","resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.048626\n","resetting env. episode 9.000000, reward total was -20.000000. running mean: -20.048139\n","resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.057658\n","resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.067081\n","resetting env. episode 12.000000, reward total was -19.000000. running mean: -20.056411\n","resetting env. episode 13.000000, reward total was -21.000000. running mean: -20.065847\n","resetting env. episode 14.000000, reward total was -19.000000. running mean: -20.055188\n","resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.064636\n","resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.063990\n","resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.053350\n","resetting env. episode 18.000000, reward total was -19.000000. running mean: -20.042816\n","resetting env. episode 19.000000, reward total was -20.000000. running mean: -20.042388\n","resetting env. episode 20.000000, reward total was -21.000000. running mean: -20.051964\n","resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.061445\n","resetting env. episode 22.000000, reward total was -18.000000. running mean: -20.040830\n","resetting env. episode 23.000000, reward total was -21.000000. running mean: -20.050422\n","resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.059918\n","resetting env. episode 25.000000, reward total was -20.000000. running mean: -20.059319\n","resetting env. episode 26.000000, reward total was -19.000000. running mean: -20.048725\n","resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.048238\n","resetting env. episode 28.000000, reward total was -21.000000. running mean: -20.057756\n","resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.067178\n","resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.076506\n","resetting env. episode 31.000000, reward total was -21.000000. running mean: -20.085741\n","resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.094884\n","resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.103935\n","resetting env. episode 34.000000, reward total was -21.000000. running mean: -20.112896\n","resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.121767\n","resetting env. episode 36.000000, reward total was -20.000000. running mean: -20.120549\n","resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.129344\n","resetting env. episode 38.000000, reward total was -19.000000. running mean: -20.118050\n","resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.126870\n","resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.135601\n","resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.144245\n","resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.152803\n","resetting env. episode 43.000000, reward total was -21.000000. running mean: -20.161275\n","resetting env. episode 44.000000, reward total was -21.000000. running mean: -20.169662\n","resetting env. episode 45.000000, reward total was -21.000000. running mean: -20.177965\n","resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.186186\n","resetting env. episode 47.000000, reward total was -20.000000. running mean: -20.184324\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m<ipython-input-16-2d6594bc5b10>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(env, model, total_episodes)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mepdlogp\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0mdiscounted_epr\u001b[0m \u001b[0;31m# modulate the gradient with advantage (PG magic happens right here.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m       \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepdlogp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgrad_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# accumulate grad over batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-16-2d6594bc5b10>\u001b[0m in \u001b[0;36mpolicy_backward\u001b[0;34m(epx, eph, epdlogp)\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mdh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mouter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepdlogp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m   \u001b[0mdh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0meph\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# backpro prelu\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m   \u001b[0mdW1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdW1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'W2'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdW2\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"id":"ZJUybWUALvQz","outputId":"0b5fec06-a0e1-4679-c641-d30412c2b097","colab":{"base_uri":"https://localhost:8080/"}},"cell_type":"code","source":["play_game(env, model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["Episode finished without success, accumulated reward = -13.0\n"]}]},{"metadata":{"id":"cHYCDYwhlVLV"},"cell_type":"code","source":["%time hist2 = train_model(env, model, total_episodes=1000)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"8fheN9DRlWXQ"},"cell_type":"code","source":["play_game(env, model)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"9AxOcQhIsKow"},"cell_type":"code","source":["%time hist3 = train_model(env, model, total_episodes=1000)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"w2NblmwDsL3y"},"cell_type":"code","source":["play_game(env, model)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"ZYA0HgMoO77a"},"cell_type":"code","source":["# import pickle\n","# pickle.dump(model, open('model.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"2Vu9PonFR5NA"},"cell_type":"code","source":[""],"execution_count":null,"outputs":[]}]}