{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of pg-from-scratch.ipynb","provenance":[{"file_id":"https://github.com/DJCordhose/ai/blob/master/notebooks/rl/pg-from-scratch.ipynb","timestamp":1660966931391}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Pong game with varying neurons and learning rate**"],"metadata":{"id":"ZHYOZhSUGJ9h"}},{"metadata":{"id":"uF9MAVI16huj"},"cell_type":"code","source":["# !apt-get install python-opengl -y  >/dev/null\n","# !apt install xvfb -y >/dev/null"],"execution_count":null,"outputs":[]},{"metadata":{"id":"fSC11TfN6p69"},"cell_type":"code","source":["# !pip install pyvirtualdisplay >/dev/null\n","# !pip install piglet >/dev/null"],"execution_count":null,"outputs":[]},{"metadata":{"id":"caiHE2hy6xrf"},"cell_type":"code","source":["# from pyvirtualdisplay import Display\n","# display = Display(visible=0, size=(1400, 900))\n","# display.start()"],"execution_count":null,"outputs":[]},{"metadata":{"id":"cWACPRL869I4"},"cell_type":"code","source":["!pip install gym >/dev/null"],"execution_count":null,"outputs":[]},{"metadata":{"id":"2Os6feRY6ec_"},"cell_type":"code","source":["!pip install JSAnimation >/dev/null"],"execution_count":null,"outputs":[]},{"metadata":{"id":"wotUOa_e6edP"},"cell_type":"code","source":["%matplotlib inline\n","from JSAnimation.IPython_display import display_animation\n","from matplotlib import animation\n","import matplotlib.pyplot as plt\n","from IPython.display import HTML\n","\n","def display_frames_as_gif(frames):\n","    \"\"\"\n","    Displays a list of frames as a gif, with controls\n","    \"\"\"\n","    plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 144)\n","    patch = plt.imshow(frames[0])\n","    plt.axis('off')\n","\n","    def animate(i):\n","        patch.set_data(frames[i])\n","\n","    anim = animation.FuncAnimation(plt.gcf(), animate, frames = len(frames), interval=50)\n","    display(display_animation(anim, default_mode='once'))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"R66_INeZ9nYX"},"cell_type":"markdown","source":["## Step 2: Playing Pong"]},{"metadata":{"id":"MtT2GyK_6edc","outputId":"00945159-b40d-4a69-b5c8-38da37bd37fb","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660966068377,"user_tz":240,"elapsed":31241,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["%pip install -U gym>=0.21.0\n","%pip install -U gym[atari,accept-rom-license]\n","import gym\n","env = gym.make('Pong-v0')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gym[accept-rom-license,atari] in /usr/local/lib/python3.7/dist-packages (0.25.2)\n","Requirement already satisfied: importlib-metadata>=4.8.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (4.12.0)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.21.6)\n","Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (1.5.0)\n","Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.7/dist-packages (from gym[accept-rom-license,atari]) (0.0.8)\n","Collecting autorom[accept-rom-license]~=0.4.2\n","  Downloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n","Collecting ale-py~=0.7.5\n","  Downloading ale_py-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 4.7 MB/s \n","\u001b[?25hRequirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from ale-py~=0.7.5->gym[accept-rom-license,atari]) (5.9.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (7.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.23.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (4.64.0)\n","Collecting AutoROM.accept-rom-license\n","  Downloading AutoROM.accept-rom-license-0.4.2.tar.gz (9.8 kB)\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.8.0->gym[accept-rom-license,atari]) (4.1.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->autorom[accept-rom-license]~=0.4.2->gym[accept-rom-license,atari]) (2.10)\n","Building wheels for collected packages: AutoROM.accept-rom-license\n","  Building wheel for AutoROM.accept-rom-license (PEP 517) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.4.2-py3-none-any.whl size=441027 sha256=bd2ae7c8801f81a93e02cb164e90ef2a82e87f386b7cff07b6ddc2d943b93503\n","  Stored in directory: /root/.cache/pip/wheels/87/67/2e/6147e7912fe37f5408b80d07527dab807c1d25f5c403a9538a\n","Successfully built AutoROM.accept-rom-license\n","Installing collected packages: AutoROM.accept-rom-license, autorom, ale-py\n","Successfully installed AutoROM.accept-rom-license-0.4.2 ale-py-0.7.5 autorom-0.4.2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/envs/registration.py:594: UserWarning: \u001b[33mWARN: The environment Pong-v0 is out of date. You should consider upgrading to version `v4`.\u001b[0m\n","  f\"The environment {id} is out of date. You should consider \"\n","/usr/local/lib/python3.7/dist-packages/gym/core.py:318: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n","/usr/local/lib/python3.7/dist-packages/gym/wrappers/step_api_compatibility.py:40: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n","  \"Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\"\n"]}]},{"metadata":{"id":"oRE6WmXQJ1Z0","outputId":"30ab574e-3248-4289-f85d-9eda51549268","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660966068386,"user_tz":240,"elapsed":106,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["env.action_space"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Discrete(6)"]},"metadata":{},"execution_count":8}]},{"metadata":{"id":"yl_9d4HFJ31W","outputId":"09b9db3f-6725-47c9-b885-ea6fac9f7db1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660966068392,"user_tz":240,"elapsed":87,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["env.observation_space"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(0, 255, (210, 160, 3), uint8)"]},"metadata":{},"execution_count":9}]},{"metadata":{"id":"trwRXI-h6eeI","outputId":"6e2c3794-a780-4328-a6f1-19522814ce27","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660966068904,"user_tz":240,"elapsed":573,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["# Run a demo of the environment\n","observation = env.reset()\n","cumulated_reward = 0\n","\n","frames = []\n","for t in range(1000):\n","#     print(observation)\n","    frames.append(env.render(mode = 'rgb_array'))\n","    # very stupid agent, just makes a random action within the allowd action space\n","    action = env.action_space.sample()\n","#     print(\"Action: {}\".format(t+1))    \n","    observation, reward, done, info = env.step(action)\n","#     print(reward)\n","    cumulated_reward += reward\n","    if done:\n","        print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n","        break\n","print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n","\n","env.close()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gym/core.py:44: DeprecationWarning: \u001b[33mWARN: The argument mode in render method is deprecated; use render_mode during environment initialization instead.\n","See here for more information: https://www.gymlibrary.ml/content/api/\u001b[0m\n","  \"The argument mode in render method is deprecated; \"\n","/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:298: UserWarning: \u001b[33mWARN: No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\u001b[0m\n","  \"No render fps was declared in the environment (env.metadata['render_fps'] is None or not defined), rendering may occur at inconsistent fps.\"\n","/usr/local/lib/python3.7/dist-packages/gym/utils/passive_env_checker.py:228: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","  \"Core environment is written in old step API which returns one bool instead of two. \"\n"]},{"output_type":"stream","name":"stdout","text":["Episode finished without success, accumulated reward = -12.0\n"]}]},{"metadata":{"id":"qUNldrqa6eeM"},"cell_type":"code","source":[""],"execution_count":null,"outputs":[]},{"metadata":{"id":"3zZTecVWLLes"},"cell_type":"code","source":["def sigmoid(x): \n","  return 1.0 / (1.0 + np.exp(-x)) # sigmoid \"squashing\" function to interval [0,1]\n","\n","def prepro(I):\n","  \"\"\" prepro 210x160x3 uint8 frame into 6400 (80x80) 1D float vector \"\"\"\n","  I = I[35:195] # crop\n","  I = I[::2,::2,0] # downsample by factor of 2\n","  I[I == 144] = 0 # erase background (background type 1)\n","  I[I == 109] = 0 # erase background (background type 2)\n","  I[I != 0] = 1 # everything else (paddles, ball) just set to 1\n","  return I.astype(np.float).ravel()\n","\n","def policy_forward(x):\n","  h = np.dot(model['W1'], x)\n","  h[h<0] = 0 # ReLU nonlinearity\n","  logp = np.dot(model['W2'], h)\n","  p = sigmoid(logp)\n","  return p, h # return probability of taking action 2, and hidden state\n","\n","def model_step(model, observation, prev_x):\n","  # preprocess the observation, set input to network to be difference image\n","  cur_x = prepro(observation)\n","  x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","  prev_x = cur_x\n","  \n","  # forward the policy network and sample an action from the returned probability\n","  aprob, _ = policy_forward(x)\n","  action = 2 if aprob >= 0.5 else 3 # roll the dice!\n","  \n","  return action, prev_x\n","\n","def play_game(env, model):\n","  observation = env.reset()\n","\n","  frames = []\n","  cumulated_reward = 0\n","\n","  prev_x = None # used in computing the difference frame\n","\n","  for t in range(1000):\n","      frames.append(env.render(mode = 'rgb_array'))\n","      action, prev_x = model_step(model, observation, prev_x)\n","      observation, reward, done, info = env.step(action)\n","      cumulated_reward += reward\n","      if done:\n","          print(\"Episode finished after {} timesteps, accumulated reward = {}\".format(t+1, cumulated_reward))\n","          break\n","  print(\"Episode finished without success, accumulated reward = {}\".format(cumulated_reward))\n","  env.close()\n","  display_frames_as_gif(frames)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"6gWvZQ7AQLQt"},"cell_type":"markdown","source":["## Step 3: Policy Gradient from Scratch"]},{"metadata":{"id":"eqFm7hqcItWl"},"cell_type":"code","source":["import numpy as np\n","\n","# model initialization\n","H = 800 # number of hidden layer neurons\n","D = 80 * 80 # input dimensionality: 80x80 grid\n","model = {}\n","model['W1'] = np.random.randn(H,D) / np.sqrt(D) # \"Xavier\" initialization\n","model['W2'] = np.random.randn(H) / np.sqrt(H)\n","\n","# import pickle\n","# model = pickle.load(open('model.pkl', 'rb'))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"IJ4SeY8tPxvn"},"cell_type":"code","source":[""],"execution_count":null,"outputs":[]},{"metadata":{"id":"TwjiwKisQM19"},"cell_type":"code","source":["# hyperparameters\n","batch_size = 10 # every how many episodes to do a param update?\n","# learning_rate = 1e-4\n","learning_rate = 1e-4\n"," \n","gamma = 0.99 # discount factor for reward\n","decay_rate = 0.99 # decay factor for RMSProp leaky sum of grad^2\n","  \n","grad_buffer = { k : np.zeros_like(v) for k,v in model.items() } # update buffers that add up gradients over a batch\n","rmsprop_cache = { k : np.zeros_like(v) for k,v in model.items() } # rmsprop memory\n","\n","def discount_rewards(r):\n","  \"\"\" take 1D float array of rewards and compute discounted reward \"\"\"\n","  discounted_r = np.zeros_like(r, dtype=np.float32)\n","  running_add = 0\n","  for t in reversed(range(0, r.size)):\n","    if r[t] != 0: running_add = 0 # reset the sum, since this was a game boundary (pong specific!)\n","    running_add = running_add * gamma + r[t]\n","    discounted_r[t] = running_add\n","  return discounted_r\n","\n","def policy_backward(epx, eph, epdlogp):\n","  \"\"\" backward pass. (eph is array of intermediate hidden states) \"\"\"\n","  dW2 = np.dot(eph.T, epdlogp).ravel()\n","  dh = np.outer(epdlogp, model['W2'])\n","  dh[eph <= 0] = 0 # backpro prelu\n","  dW1 = np.dot(dh.T, epx)\n","  return {'W1':dW1, 'W2':dW2}\n","\n","def train_model(env, model, total_episodes = 100):\n","  hist = []\n","  observation = env.reset()\n","\n","  prev_x = None # used in computing the difference frame\n","  xs,hs,dlogps,drs = [],[],[],[]\n","  running_reward = None\n","  reward_sum = 0\n","  episode_number = 0\n","\n","  while True:\n","    # preprocess the observation, set input to network to be difference image\n","    cur_x = prepro(observation)\n","    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n","    prev_x = cur_x\n","\n","    # forward the policy network and sample an action from the returned probability\n","    aprob, h = policy_forward(x)\n","    action = 2 if np.random.uniform() < aprob else 3 # roll the dice!\n","\n","    # record various intermediates (needed later for backprop)\n","    xs.append(x) # observation\n","    hs.append(h) # hidden state\n","    y = 1 if action == 2 else 0 # a \"fake label\"\n","    dlogps.append(y - aprob) # grad that encourages the action that was taken to be taken (see http://cs231n.github.io/neural-networks-2/#losses if confused)\n","\n","    # step the environment and get new measurements\n","    observation, reward, done, info = env.step(action)\n","    reward_sum += reward\n","\n","    drs.append(reward) # record reward (has to be done after we call step() to get reward for previous action)\n","\n","    if done: # an episode finished\n","      episode_number += 1\n","\n","      # stack together all inputs, hidden states, action gradients, and rewards for this episode\n","      epx = np.vstack(xs)\n","      eph = np.vstack(hs)\n","      epdlogp = np.vstack(dlogps)\n","      epr = np.vstack(drs)\n","      xs,hs,dlogps,drs = [],[],[],[] # reset array memory\n","\n","      # compute the discounted reward backwards through time\n","      discounted_epr = discount_rewards(epr)\n","      # standardize the rewards to be unit normal (helps control the gradient estimator variance)\n","      discounted_epr -= np.mean(discounted_epr)\n","      discounted_epr /= np.std(discounted_epr)\n","\n","      epdlogp *= discounted_epr # modulate the gradient with advantage (PG magic happens right here.)\n","      grad = policy_backward(epx, eph, epdlogp)\n","      for k in model: grad_buffer[k] += grad[k] # accumulate grad over batch\n","\n","      # perform rmsprop parameter update every batch_size episodes\n","      if episode_number % batch_size == 0:\n","        for k,v in model.items():\n","          g = grad_buffer[k] # gradient\n","          rmsprop_cache[k] = decay_rate * rmsprop_cache[k] + (1 - decay_rate) * g**2\n","          model[k] += learning_rate * g / (np.sqrt(rmsprop_cache[k]) + 1e-5)\n","          grad_buffer[k] = np.zeros_like(v) # reset batch gradient buffer\n","\n","      # boring book-keeping\n","      running_reward = reward_sum if running_reward is None else running_reward * 0.99 + reward_sum * 0.01\n","      hist.append((episode_number, reward_sum, running_reward))\n","      plt.scatter(episode_number, running_reward, marker=\".\")\n","      print ('resetting env. episode %f, reward total was %f. running mean: %f' % (episode_number, reward_sum, running_reward))\n","\n","      reward_sum = 0\n","      observation = env.reset() # reset env\n","      prev_x = None\n","      if episode_number == total_episodes or running_reward >= -10: \n","        return hist\n","        return plt.show()\n","\n","  #   if reward != 0: # Pong has either +1 or -1 reward exactly when game ends.\n","  #     print (('ep %d: game finished, reward: %f' % (episode_number, reward)) + ('' if reward == -1 else ' !!!!!!!!'))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"G6Ka_5Vl9Orm","outputId":"7b08dbbf-424e-4bf4-a79f-b226a3067c0d","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1660966864366,"user_tz":240,"elapsed":795121,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["%time hist1 = train_model(env, model, total_episodes=4000)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["resetting env. episode 1.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 2.000000, reward total was -21.000000. running mean: -21.000000\n","resetting env. episode 3.000000, reward total was -20.000000. running mean: -20.990000\n","resetting env. episode 4.000000, reward total was -21.000000. running mean: -20.990100\n","resetting env. episode 5.000000, reward total was -21.000000. running mean: -20.990199\n","resetting env. episode 6.000000, reward total was -18.000000. running mean: -20.960297\n","resetting env. episode 7.000000, reward total was -20.000000. running mean: -20.950694\n","resetting env. episode 8.000000, reward total was -21.000000. running mean: -20.951187\n","resetting env. episode 9.000000, reward total was -21.000000. running mean: -20.951675\n","resetting env. episode 10.000000, reward total was -21.000000. running mean: -20.952158\n","resetting env. episode 11.000000, reward total was -21.000000. running mean: -20.952637\n","resetting env. episode 12.000000, reward total was -21.000000. running mean: -20.953111\n","resetting env. episode 13.000000, reward total was -19.000000. running mean: -20.933579\n","resetting env. episode 14.000000, reward total was -17.000000. running mean: -20.894244\n","resetting env. episode 15.000000, reward total was -21.000000. running mean: -20.895301\n","resetting env. episode 16.000000, reward total was -20.000000. running mean: -20.886348\n","resetting env. episode 17.000000, reward total was -19.000000. running mean: -20.867485\n","resetting env. episode 18.000000, reward total was -21.000000. running mean: -20.868810\n","resetting env. episode 19.000000, reward total was -21.000000. running mean: -20.870122\n","resetting env. episode 20.000000, reward total was -20.000000. running mean: -20.861421\n","resetting env. episode 21.000000, reward total was -21.000000. running mean: -20.862806\n","resetting env. episode 22.000000, reward total was -21.000000. running mean: -20.864178\n","resetting env. episode 23.000000, reward total was -19.000000. running mean: -20.845536\n","resetting env. episode 24.000000, reward total was -21.000000. running mean: -20.847081\n","resetting env. episode 25.000000, reward total was -21.000000. running mean: -20.848610\n","resetting env. episode 26.000000, reward total was -21.000000. running mean: -20.850124\n","resetting env. episode 27.000000, reward total was -20.000000. running mean: -20.841623\n","resetting env. episode 28.000000, reward total was -19.000000. running mean: -20.823207\n","resetting env. episode 29.000000, reward total was -21.000000. running mean: -20.824975\n","resetting env. episode 30.000000, reward total was -21.000000. running mean: -20.826725\n","resetting env. episode 31.000000, reward total was -20.000000. running mean: -20.818458\n","resetting env. episode 32.000000, reward total was -21.000000. running mean: -20.820273\n","resetting env. episode 33.000000, reward total was -21.000000. running mean: -20.822070\n","resetting env. episode 34.000000, reward total was -20.000000. running mean: -20.813850\n","resetting env. episode 35.000000, reward total was -21.000000. running mean: -20.815711\n","resetting env. episode 36.000000, reward total was -19.000000. running mean: -20.797554\n","resetting env. episode 37.000000, reward total was -21.000000. running mean: -20.799579\n","resetting env. episode 38.000000, reward total was -20.000000. running mean: -20.791583\n","resetting env. episode 39.000000, reward total was -21.000000. running mean: -20.793667\n","resetting env. episode 40.000000, reward total was -21.000000. running mean: -20.795730\n","resetting env. episode 41.000000, reward total was -21.000000. running mean: -20.797773\n","resetting env. episode 42.000000, reward total was -21.000000. running mean: -20.799795\n","resetting env. episode 43.000000, reward total was -20.000000. running mean: -20.791797\n","resetting env. episode 44.000000, reward total was -20.000000. running mean: -20.783879\n","resetting env. episode 45.000000, reward total was -20.000000. running mean: -20.776040\n","resetting env. episode 46.000000, reward total was -21.000000. running mean: -20.778280\n","resetting env. episode 47.000000, reward total was -21.000000. running mean: -20.780497\n","resetting env. episode 48.000000, reward total was -21.000000. running mean: -20.782692\n","resetting env. episode 49.000000, reward total was -19.000000. running mean: -20.764865\n","resetting env. episode 50.000000, reward total was -21.000000. running mean: -20.767217\n","resetting env. episode 51.000000, reward total was -21.000000. running mean: -20.769545\n","resetting env. episode 52.000000, reward total was -20.000000. running mean: -20.761849\n","resetting env. episode 53.000000, reward total was -21.000000. running mean: -20.764231\n","resetting env. episode 54.000000, reward total was -19.000000. running mean: -20.746588\n","resetting env. episode 55.000000, reward total was -19.000000. running mean: -20.729122\n","resetting env. episode 56.000000, reward total was -21.000000. running mean: -20.731831\n","resetting env. episode 57.000000, reward total was -21.000000. running mean: -20.734513\n","resetting env. episode 58.000000, reward total was -21.000000. running mean: -20.737168\n","resetting env. episode 59.000000, reward total was -18.000000. running mean: -20.709796\n","resetting env. episode 60.000000, reward total was -21.000000. running mean: -20.712698\n","resetting env. episode 61.000000, reward total was -20.000000. running mean: -20.705571\n","resetting env. episode 62.000000, reward total was -20.000000. running mean: -20.698515\n","resetting env. episode 63.000000, reward total was -20.000000. running mean: -20.691530\n","resetting env. episode 64.000000, reward total was -20.000000. running mean: -20.684615\n","resetting env. episode 65.000000, reward total was -21.000000. running mean: -20.687769\n","resetting env. episode 66.000000, reward total was -21.000000. running mean: -20.690891\n","resetting env. episode 67.000000, reward total was -21.000000. running mean: -20.693982\n","resetting env. episode 68.000000, reward total was -21.000000. running mean: -20.697042\n","resetting env. episode 69.000000, reward total was -21.000000. running mean: -20.700072\n","resetting env. episode 70.000000, reward total was -21.000000. running mean: -20.703071\n","resetting env. episode 71.000000, reward total was -21.000000. running mean: -20.706041\n","resetting env. episode 72.000000, reward total was -21.000000. running mean: -20.708980\n","resetting env. episode 73.000000, reward total was -20.000000. running mean: -20.701890\n","resetting env. episode 74.000000, reward total was -21.000000. running mean: -20.704871\n","resetting env. episode 75.000000, reward total was -21.000000. running mean: -20.707823\n","resetting env. episode 76.000000, reward total was -19.000000. running mean: -20.690744\n","resetting env. episode 77.000000, reward total was -19.000000. running mean: -20.673837\n","resetting env. episode 78.000000, reward total was -20.000000. running mean: -20.667099\n","resetting env. episode 79.000000, reward total was -19.000000. running mean: -20.650428\n","resetting env. episode 80.000000, reward total was -21.000000. running mean: -20.653923\n","resetting env. episode 81.000000, reward total was -20.000000. running mean: -20.647384\n","resetting env. episode 82.000000, reward total was -21.000000. running mean: -20.650910\n","resetting env. episode 83.000000, reward total was -19.000000. running mean: -20.634401\n","resetting env. episode 84.000000, reward total was -21.000000. running mean: -20.638057\n","resetting env. episode 85.000000, reward total was -20.000000. running mean: -20.631677\n","resetting env. episode 86.000000, reward total was -21.000000. running mean: -20.635360\n","resetting env. episode 87.000000, reward total was -18.000000. running mean: -20.609006\n","resetting env. episode 88.000000, reward total was -21.000000. running mean: -20.612916\n","resetting env. episode 89.000000, reward total was -21.000000. running mean: -20.616787\n","resetting env. episode 90.000000, reward total was -20.000000. running mean: -20.610619\n","resetting env. episode 91.000000, reward total was -21.000000. running mean: -20.614513\n","resetting env. episode 92.000000, reward total was -21.000000. running mean: -20.618368\n","resetting env. episode 93.000000, reward total was -20.000000. running mean: -20.612184\n","resetting env. episode 94.000000, reward total was -21.000000. running mean: -20.616062\n","resetting env. episode 95.000000, reward total was -21.000000. running mean: -20.619902\n","resetting env. episode 96.000000, reward total was -20.000000. running mean: -20.613703\n","resetting env. episode 97.000000, reward total was -19.000000. running mean: -20.597566\n","resetting env. episode 98.000000, reward total was -21.000000. running mean: -20.601590\n","resetting env. episode 99.000000, reward total was -21.000000. running mean: -20.605574\n","resetting env. episode 100.000000, reward total was -20.000000. running mean: -20.599518\n","resetting env. episode 101.000000, reward total was -21.000000. running mean: -20.603523\n","resetting env. episode 102.000000, reward total was -21.000000. running mean: -20.607488\n","resetting env. episode 103.000000, reward total was -20.000000. running mean: -20.601413\n","resetting env. episode 104.000000, reward total was -21.000000. running mean: -20.605399\n","resetting env. episode 105.000000, reward total was -20.000000. running mean: -20.599345\n","resetting env. episode 106.000000, reward total was -21.000000. running mean: -20.603352\n","resetting env. episode 107.000000, reward total was -21.000000. running mean: -20.607318\n","resetting env. episode 108.000000, reward total was -20.000000. running mean: -20.601245\n","resetting env. episode 109.000000, reward total was -21.000000. running mean: -20.605232\n","resetting env. episode 110.000000, reward total was -18.000000. running mean: -20.579180\n","resetting env. episode 111.000000, reward total was -21.000000. running mean: -20.583388\n","resetting env. episode 112.000000, reward total was -21.000000. running mean: -20.587554\n","resetting env. episode 113.000000, reward total was -20.000000. running mean: -20.581679\n","resetting env. episode 114.000000, reward total was -21.000000. running mean: -20.585862\n","resetting env. episode 115.000000, reward total was -21.000000. running mean: -20.590003\n","resetting env. episode 116.000000, reward total was -21.000000. running mean: -20.594103\n","resetting env. episode 117.000000, reward total was -21.000000. running mean: -20.598162\n","resetting env. episode 118.000000, reward total was -21.000000. running mean: -20.602181\n","resetting env. episode 119.000000, reward total was -21.000000. running mean: -20.606159\n","resetting env. episode 120.000000, reward total was -21.000000. running mean: -20.610097\n","resetting env. episode 121.000000, reward total was -21.000000. running mean: -20.613996\n","resetting env. episode 122.000000, reward total was -20.000000. running mean: -20.607856\n","resetting env. episode 123.000000, reward total was -20.000000. running mean: -20.601778\n","resetting env. episode 124.000000, reward total was -20.000000. running mean: -20.595760\n","resetting env. episode 125.000000, reward total was -21.000000. running mean: -20.599802\n","resetting env. episode 126.000000, reward total was -21.000000. running mean: -20.603804\n","resetting env. episode 127.000000, reward total was -20.000000. running mean: -20.597766\n","resetting env. episode 128.000000, reward total was -21.000000. running mean: -20.601789\n","resetting env. episode 129.000000, reward total was -21.000000. running mean: -20.605771\n","resetting env. episode 130.000000, reward total was -21.000000. running mean: -20.609713\n","resetting env. episode 131.000000, reward total was -21.000000. running mean: -20.613616\n","resetting env. episode 132.000000, reward total was -20.000000. running mean: -20.607480\n","resetting env. episode 133.000000, reward total was -20.000000. running mean: -20.601405\n","resetting env. episode 134.000000, reward total was -21.000000. running mean: -20.605391\n","resetting env. episode 135.000000, reward total was -21.000000. running mean: -20.609337\n","resetting env. episode 136.000000, reward total was -20.000000. running mean: -20.603244\n","resetting env. episode 137.000000, reward total was -21.000000. running mean: -20.607211\n","resetting env. episode 138.000000, reward total was -20.000000. running mean: -20.601139\n","resetting env. episode 139.000000, reward total was -20.000000. running mean: -20.595128\n","resetting env. episode 140.000000, reward total was -19.000000. running mean: -20.579176\n","resetting env. episode 141.000000, reward total was -21.000000. running mean: -20.583385\n","resetting env. episode 142.000000, reward total was -21.000000. running mean: -20.587551\n","resetting env. episode 143.000000, reward total was -17.000000. running mean: -20.551675\n","resetting env. episode 144.000000, reward total was -21.000000. running mean: -20.556159\n","resetting env. episode 145.000000, reward total was -20.000000. running mean: -20.550597\n","resetting env. episode 146.000000, reward total was -21.000000. running mean: -20.555091\n","resetting env. episode 147.000000, reward total was -20.000000. running mean: -20.549540\n","resetting env. episode 148.000000, reward total was -20.000000. running mean: -20.544045\n","resetting env. episode 149.000000, reward total was -21.000000. running mean: -20.548604\n","resetting env. episode 150.000000, reward total was -21.000000. running mean: -20.553118\n","resetting env. episode 151.000000, reward total was -20.000000. running mean: -20.547587\n","resetting env. episode 152.000000, reward total was -21.000000. running mean: -20.552111\n","resetting env. episode 153.000000, reward total was -21.000000. running mean: -20.556590\n","resetting env. episode 154.000000, reward total was -21.000000. running mean: -20.561024\n","resetting env. episode 155.000000, reward total was -20.000000. running mean: -20.555414\n","resetting env. episode 156.000000, reward total was -20.000000. running mean: -20.549860\n","resetting env. episode 157.000000, reward total was -19.000000. running mean: -20.534361\n","resetting env. episode 158.000000, reward total was -17.000000. running mean: -20.499018\n","resetting env. episode 159.000000, reward total was -20.000000. running mean: -20.494027\n","resetting env. episode 160.000000, reward total was -20.000000. running mean: -20.489087\n","resetting env. episode 161.000000, reward total was -21.000000. running mean: -20.494196\n","resetting env. episode 162.000000, reward total was -20.000000. running mean: -20.489254\n","resetting env. episode 163.000000, reward total was -21.000000. running mean: -20.494362\n","resetting env. episode 164.000000, reward total was -21.000000. running mean: -20.499418\n","resetting env. episode 165.000000, reward total was -21.000000. running mean: -20.504424\n","resetting env. episode 166.000000, reward total was -21.000000. running mean: -20.509380\n","resetting env. episode 167.000000, reward total was -20.000000. running mean: -20.504286\n","resetting env. episode 168.000000, reward total was -21.000000. running mean: -20.509243\n","resetting env. episode 169.000000, reward total was -20.000000. running mean: -20.504151\n","resetting env. episode 170.000000, reward total was -21.000000. running mean: -20.509109\n","resetting env. episode 171.000000, reward total was -20.000000. running mean: -20.504018\n","resetting env. episode 172.000000, reward total was -21.000000. running mean: -20.508978\n","resetting env. episode 173.000000, reward total was -20.000000. running mean: -20.503888\n","resetting env. episode 174.000000, reward total was -20.000000. running mean: -20.498849\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n","\u001b[0;32m<ipython-input-13-16d86b44a4fb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(env, model, total_episodes)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;31m# forward the policy network and sample an action from the returned probability\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0maprob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0maprob\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;31m# roll the dice!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-11-6efbf329445c>\u001b[0m in \u001b[0;36mpolicy_forward\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mpolicy_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0mh\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m<\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;31m# ReLU nonlinearity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0mlogp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'W2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"id":"ZJUybWUALvQz","outputId":"792e0647-1ad4-4a45-b100-18d8e2fceeb9","colab":{"base_uri":"https://localhost:8080/","height":410},"executionInfo":{"status":"error","timestamp":1660966905944,"user_tz":240,"elapsed":41601,"user":{"displayName":"","userId":""}}},"cell_type":"code","source":["play_game(env, model)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:11: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n","Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n","  # This is added back by InteractiveShellApp.init_path()\n"]},{"output_type":"stream","name":"stdout","text":["Episode finished without success, accumulated reward = -10.0\n"]},{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-520373e4964a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplay_game\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-11-6efbf329445c>\u001b[0m in \u001b[0;36mplay_game\u001b[0;34m(env, model)\u001b[0m\n\u001b[1;32m     48\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Episode finished without success, accumulated reward = {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcumulated_reward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m   \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m   \u001b[0mdisplay_frames_as_gif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-6-0fb57280a5cd>\u001b[0m in \u001b[0;36mdisplay_frames_as_gif\u001b[0;34m(frames)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0manim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFuncAnimation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgcf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manimate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisplay_animation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'once'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36mdisplay_animation\u001b[0;34m(anim, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;34m\"\"\"Display the animation with an IPython HTML object\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mHTML\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim_to_html\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/JSAnimation/IPython_display.py\u001b[0m in \u001b[0;36manim_to_html\u001b[0;34m(anim, fps, embed_frames, default_mode)\u001b[0m\n\u001b[1;32m     74\u001b[0m             anim.save(f.name,  writer=HTMLWriter(fps=fps,\n\u001b[1;32m     75\u001b[0m                                                  \u001b[0membed_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membed_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                                                  default_mode=default_mode))\n\u001b[0m\u001b[1;32m     77\u001b[0m             \u001b[0mhtml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1150\u001b[0m                             \u001b[0mprogress_callback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_number\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_frames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m                             \u001b[0mframe_number\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m                     \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrab_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0msavefig_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;31m# Reconnect signal for first draw if necessary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36mfinish\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;31m# Call run here now that all frame grabbing is done. All temp files\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;31m# are available to be assembled.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m         \u001b[0mMovieWriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Will call clean-up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/JSAnimation/html_writer.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0mof\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mJS_INCLUDE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             of.write(DISPLAY_TEMPLATE.format(id=self.new_id(),\n\u001b[0;32m--> 323\u001b[0;31m                                              \u001b[0mNframes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_temp_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m                                              \u001b[0mfill_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_frames\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                                              \u001b[0minterval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minterval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'HTMLWriter' object has no attribute '_temp_names'"]}]},{"metadata":{"id":"cHYCDYwhlVLV"},"cell_type":"code","source":["%time hist2 = train_model(env, model, total_episodes=1000)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"8fheN9DRlWXQ"},"cell_type":"code","source":["play_game(env, model)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"9AxOcQhIsKow"},"cell_type":"code","source":["%time hist3 = train_model(env, model, total_episodes=1000)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"w2NblmwDsL3y"},"cell_type":"code","source":["play_game(env, model)"],"execution_count":null,"outputs":[]},{"metadata":{"id":"ZYA0HgMoO77a"},"cell_type":"code","source":["# import pickle\n","# pickle.dump(model, open('model.pkl', 'wb'))"],"execution_count":null,"outputs":[]},{"metadata":{"id":"2Vu9PonFR5NA"},"cell_type":"code","source":[""],"execution_count":null,"outputs":[]}]}